{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522e7758-7ffd-4d9b-92f4-487d1ae9d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import autograd_lib as agl\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.autograd import Variable\n",
    "#from functorch import hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51edd55-16dd-4258-aa93-eb8d84198af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d25e4bf-ce13-4d2e-9a57-477605a8058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.expand_dims(np.arange(-1.5, 1.5, 0.01),1) \n",
    "print(x.shape)\n",
    "y = np.sinc(5*x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d06881-ac24-4c4a-abb0-6e94cb383c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78818ee3-451c-4a83-9b22-1b8ff16c2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Model1, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 5)\n",
    "        self.linear2 = nn.Linear(5, 10)\n",
    "        self.linear3 = nn.Linear(10, 10)\n",
    "        self.linear4 = nn.Linear(10, 10)\n",
    "        self.linear5 = nn.Linear(10, 10)\n",
    "        self.linear6 = nn.Linear(10, 10)\n",
    "        self.linear7 = nn.Linear(10, 5)\n",
    "        self.predict = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.leaky_relu(self.linear1(x))\n",
    "        x = nn.functional.leaky_relu(self.linear2(x))\n",
    "        x = nn.functional.leaky_relu(self.linear3(x))\n",
    "        x = nn.functional.leaky_relu(self.linear4(x))\n",
    "        x = nn.functional.leaky_relu(self.linear5(x))\n",
    "        x = nn.functional.leaky_relu(self.linear6(x))\n",
    "        x = nn.functional.leaky_relu(self.linear7(x))\n",
    "\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    " \n",
    "model_1 = Model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c8d747f2-1fb2-41ef-b47e-5d1954d77a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug\n",
      "debug\n",
      "epoch: 1, loss = 0.0617\n",
      "tensor([-9.3037e-06, -1.5305e-05,  2.5873e-05,  2.8521e-05, -1.0658e-05,\n",
      "         8.8620e-06,  9.3955e-07, -9.4831e-06, -1.0660e-05, -1.1944e-05,\n",
      "         4.4630e-06,  1.8716e-07,  6.2281e-06,  3.1382e-06,  9.5444e-08,\n",
      "        -4.3666e-05, -4.4255e-05, -5.6151e-05, -2.4388e-05, -3.3672e-05,\n",
      "         1.1595e-07,  1.1648e-07,  1.4996e-07,  6.4990e-08,  8.9715e-08,\n",
      "         8.7257e-06, -2.2084e-05,  1.1140e-05,  8.2418e-06, -1.6493e-05,\n",
      "         4.8853e-06,  8.1973e-08,  6.5230e-06,  3.3561e-06,  7.0975e-10,\n",
      "        -2.3882e-07, -2.2307e-07, -3.0756e-07, -1.3550e-07, -1.7053e-07,\n",
      "         5.5956e-08,  8.6435e-08,  7.8791e-08,  2.8892e-08,  7.4277e-08,\n",
      "        -7.2276e-09, -2.2467e-09, -1.0361e-08, -4.7457e-09, -3.0632e-09,\n",
      "        -1.3792e-07, -1.9453e-07, -1.8851e-07, -7.2499e-08, -1.6184e-07,\n",
      "        -1.5421e-07, -1.7224e-07, -1.9941e-07, -8.4493e-08, -1.3247e-07,\n",
      "         6.8176e-06, -8.7374e-05,  2.3152e-07,  5.4136e-06,  7.6301e-06,\n",
      "        -4.7037e-07,  1.2305e-07, -1.2352e-08, -2.9652e-07, -3.1478e-07,\n",
      "         7.5186e-08, -6.4395e-05,  6.5757e-07, -3.2014e-05, -1.1078e-06,\n",
      "         4.1568e-07,  6.0504e-07,  2.3441e-07,  1.0501e-07,  3.4691e-07,\n",
      "         7.4563e-08, -6.3196e-05,  6.5101e-07, -3.2511e-05, -1.1017e-06,\n",
      "         4.0707e-07,  5.9256e-07,  2.3467e-07,  1.0165e-07,  3.4100e-07,\n",
      "        -6.3790e-08,  5.1491e-05, -5.5263e-07,  3.0763e-05,  9.5445e-07,\n",
      "        -3.2827e-07, -4.7805e-07, -2.0930e-07, -7.7301e-08, -2.7998e-07,\n",
      "         1.6179e-10, -1.2778e-07,  1.3969e-09, -8.1253e-08, -2.4339e-09,\n",
      "         8.1071e-10,  1.1809e-09,  5.4019e-10,  1.8549e-10,  6.9724e-10,\n",
      "         3.8291e-09,  2.2770e-07,  5.9896e-10,  2.5099e-07,  2.7390e-08,\n",
      "         2.8394e-09, -1.0089e-09,  3.2407e-09,  1.3796e-09, -1.0886e-09,\n",
      "        -4.6429e-08,  3.7114e-05, -4.0161e-07,  2.2808e-05,  6.9638e-07,\n",
      "        -2.3610e-07, -3.4386e-07, -1.5354e-07, -5.4897e-08, -2.0212e-07,\n",
      "        -4.9252e-08,  3.4934e-05, -4.1858e-07,  2.9276e-05,  7.5929e-07,\n",
      "        -2.1601e-07, -3.1497e-07, -1.7758e-07, -4.1589e-08, -1.9415e-07,\n",
      "         4.9205e-10, -3.9032e-07,  4.2512e-09, -2.4515e-07, -7.3940e-09,\n",
      "         2.4788e-09,  3.6104e-09,  1.6372e-09,  5.7052e-10,  2.1283e-09,\n",
      "         1.8098e-08, -4.3233e-05,  3.1635e-07,  3.8992e-08, -5.8284e-07,\n",
      "         2.7936e-07,  4.2538e-07,  4.9218e-08,  9.2384e-08,  2.2090e-07,\n",
      "         6.8932e-11, -5.3281e-08,  5.9320e-10, -3.5946e-08, -1.0423e-09,\n",
      "         3.3640e-10,  4.9009e-10,  2.3399e-10,  7.4676e-11,  2.9177e-10,\n",
      "        -1.1022e-04, -1.0790e-04,  8.6841e-05, -2.1427e-07, -3.8639e-07,\n",
      "         6.2434e-05,  5.6810e-05, -6.5528e-07, -7.6202e-05, -8.8829e-08,\n",
      "         5.3467e-05,  2.5378e-05,  5.7285e-05, -1.1191e-07, -7.1144e-07,\n",
      "         1.0063e-05,  4.9112e-05, -5.5597e-07,  1.2672e-05, -3.4693e-07,\n",
      "        -2.6208e-07, -1.2440e-07, -2.8080e-07,  5.4854e-10,  3.4873e-09,\n",
      "        -4.9324e-08, -2.4073e-07,  2.7252e-09, -6.2114e-08,  1.7005e-09,\n",
      "         1.3717e-07,  6.5107e-08,  1.4696e-07, -2.8710e-10, -1.8252e-09,\n",
      "         2.5815e-08,  1.2600e-07, -1.4263e-09,  3.2509e-08, -8.9003e-10,\n",
      "        -1.9693e-05, -9.3472e-06, -2.1099e-05,  4.1218e-08,  2.6204e-07,\n",
      "        -3.7062e-06, -1.8089e-05,  2.0477e-07, -4.6673e-06,  1.2778e-07,\n",
      "        -4.4141e-08, -2.0952e-08, -4.7294e-08,  9.2389e-11,  5.8735e-10,\n",
      "        -8.3076e-09, -4.0546e-08,  4.5900e-10, -1.0462e-08,  2.8642e-10,\n",
      "        -9.8020e-09, -4.6526e-09, -1.0502e-08,  2.0516e-11,  1.3043e-10,\n",
      "        -1.8448e-09, -9.0037e-09,  1.0193e-10, -2.3231e-09,  6.3602e-11,\n",
      "        -1.2906e-05, -4.3225e-06, -1.5138e-05,  4.3179e-08, -1.9063e-06,\n",
      "        -7.6908e-06, -2.5243e-05,  1.9011e-07, -2.1331e-06,  8.5491e-08,\n",
      "         1.6066e-07,  7.6259e-08,  1.7214e-07, -3.3627e-10, -2.1378e-09,\n",
      "         3.0237e-08,  1.4758e-07, -1.6706e-09,  3.8078e-08, -1.0425e-09,\n",
      "        -1.2767e-05, -6.0601e-06, -1.3679e-05,  2.6723e-08,  1.6989e-07,\n",
      "        -2.4029e-06, -1.1728e-05,  1.3276e-07, -3.0260e-06,  8.2844e-08,\n",
      "         4.0673e-05,  1.9306e-05,  4.3578e-05, -8.5130e-08, -5.4120e-07,\n",
      "         7.6548e-06,  3.7360e-05, -4.2293e-07,  9.6397e-06, -2.6391e-07,\n",
      "         2.9628e-04, -1.4523e-06,  7.6010e-07, -1.0913e-04, -2.4461e-07,\n",
      "        -5.4317e-08, -1.0587e-04,  8.9030e-07, -7.0750e-05,  2.2539e-04,\n",
      "         6.9913e-05, -2.7162e-08, -2.7779e-08,  9.7155e-05, -2.6308e-07,\n",
      "        -2.2358e-07,  1.3642e-05, -2.4906e-07,  1.6583e-05,  1.4180e-04,\n",
      "        -2.7792e-08,  1.0798e-11,  1.1043e-11, -3.8622e-08,  1.0458e-10,\n",
      "         8.8878e-11, -5.4231e-09,  9.9007e-11, -6.5920e-09, -5.6371e-08,\n",
      "        -1.0001e-07,  3.8857e-11,  3.9739e-11, -1.3899e-07,  3.7635e-10,\n",
      "         3.1984e-10, -1.9516e-08,  3.5629e-10, -2.3722e-08, -2.0286e-07,\n",
      "         2.6799e-05, -1.0412e-08, -1.0648e-08,  3.7241e-05, -1.0084e-07,\n",
      "        -8.5700e-08,  5.2292e-06, -9.5467e-08,  6.3563e-06,  5.4355e-05,\n",
      "         2.3648e-05, -9.1876e-09, -9.3960e-09,  3.2863e-05, -8.8985e-08,\n",
      "        -7.5624e-08,  4.6144e-06, -8.4243e-08,  5.6090e-06,  4.7965e-05,\n",
      "         5.6285e-05, -2.1868e-08, -2.2364e-08,  7.8217e-05, -2.1179e-07,\n",
      "        -1.7999e-07,  1.0983e-05, -2.0051e-07,  1.3350e-05,  1.1416e-04,\n",
      "         4.2405e-08, -1.6475e-11, -1.6849e-11,  5.8928e-08, -1.5956e-10,\n",
      "        -1.3561e-10,  8.2744e-09, -1.5106e-10,  1.0058e-08,  8.6009e-08,\n",
      "         8.3970e-05, -3.2624e-08, -3.3364e-08,  1.1669e-04, -3.1597e-07,\n",
      "        -2.6853e-07,  1.6385e-05, -2.9913e-07,  1.9917e-05,  1.7032e-04,\n",
      "         1.6975e-07, -6.5951e-11, -6.7447e-11,  2.3590e-07, -6.3876e-10,\n",
      "        -5.4285e-10,  3.3123e-08, -6.0472e-10,  4.0263e-08,  3.4430e-07,\n",
      "        -2.2654e-07,  8.8013e-11,  9.0009e-11, -3.1481e-07,  8.5243e-10,\n",
      "         7.2444e-10, -4.4204e-08,  8.0701e-10, -5.3732e-08, -4.5948e-07,\n",
      "         4.1162e-04, -1.6363e-07, -5.8885e-07,  1.5778e-04,  1.3923e-04,\n",
      "         3.3138e-04,  2.4966e-07,  4.9438e-04,  9.9943e-07, -1.3337e-06,\n",
      "         1.3013e-04, -1.9452e-07, -1.7961e-06,  4.5389e-05,  3.7588e-04,\n",
      "         2.3394e-04, -1.0918e-07,  2.2685e-04, -2.6202e-06, -1.7456e-06,\n",
      "         1.6113e-06, -2.4087e-09, -2.2241e-08,  5.6205e-07,  4.6545e-06,\n",
      "         2.8969e-06, -1.3519e-09,  2.8091e-06, -3.2446e-08, -2.1615e-08,\n",
      "        -2.3622e-06,  3.5312e-09,  3.2604e-08, -8.2396e-07, -6.8235e-06,\n",
      "        -4.2468e-06,  1.9819e-09, -4.1180e-06,  4.7566e-08,  3.1688e-08,\n",
      "        -2.6508e-06,  3.9626e-09,  3.6589e-08, -9.2464e-07, -7.6573e-06,\n",
      "        -4.7657e-06,  2.2241e-09, -4.6212e-06,  5.3378e-08,  3.5560e-08,\n",
      "        -7.6451e-07,  1.1428e-09,  1.0552e-08, -2.6667e-07, -2.2084e-06,\n",
      "        -1.3744e-06,  6.4143e-10, -1.3328e-06,  1.5394e-08,  1.0256e-08,\n",
      "         1.0011e-04, -1.4966e-07, -1.3818e-06,  3.4921e-05,  2.8919e-04,\n",
      "         1.7999e-04, -8.3997e-08,  1.7453e-04, -2.0159e-06, -1.3430e-06,\n",
      "        -5.9111e-07,  8.8363e-10,  8.1589e-09, -2.0619e-07, -1.7075e-06,\n",
      "        -1.0627e-06,  4.9595e-10, -1.0305e-06,  1.1903e-08,  7.9295e-09,\n",
      "         2.4477e-06, -3.6589e-09, -3.3784e-08,  8.5377e-07,  7.0704e-06,\n",
      "         4.4004e-06, -2.0536e-09,  4.2670e-06, -4.9286e-08, -3.2834e-08,\n",
      "         6.2620e-05, -9.3608e-08, -8.6431e-07,  2.1842e-05,  1.8088e-04,\n",
      "         1.1258e-04, -5.2539e-08,  1.0917e-04, -1.2609e-06, -8.4002e-07,\n",
      "         7.7511e-07, -1.1587e-09, -1.0698e-08,  2.7036e-07,  2.2390e-06,\n",
      "         1.3935e-06, -6.5032e-10,  1.3513e-06, -1.5608e-08, -1.0398e-08,\n",
      "         1.1776e-03,  1.4582e-05, -2.1377e-05, -2.3989e-05, -6.9186e-06,\n",
      "         9.0600e-04, -5.3494e-06,  2.2150e-05,  5.6669e-04,  7.0145e-06,\n",
      "        -6.1692e-05,  1.8844e-07,  1.7994e-07,  2.2930e-07,  2.7513e-08,\n",
      "        -1.7343e-05,  2.4642e-07,  1.3028e-07, -4.8048e-05,  3.6016e-08,\n",
      "         6.6980e-06, -2.0459e-08, -1.9536e-08, -2.4896e-08, -2.9871e-09,\n",
      "         1.8830e-06, -2.6754e-08, -1.4144e-08,  5.2167e-06, -3.9104e-09,\n",
      "        -2.3620e-05,  7.2150e-08,  6.8894e-08,  8.7795e-08,  1.0534e-08,\n",
      "        -6.6404e-06,  9.4348e-08,  4.9880e-08, -1.8396e-05,  1.3790e-08,\n",
      "         4.4943e-03, -1.3728e-05, -1.3109e-05, -1.6705e-05, -2.0043e-06,\n",
      "         1.2635e-03, -1.7952e-05, -9.4907e-06,  3.5003e-03, -2.6238e-06,\n",
      "         3.9828e-03, -1.2166e-05, -1.1617e-05, -1.4804e-05, -1.7762e-06,\n",
      "         1.1197e-03, -1.5909e-05, -8.4107e-06,  3.1020e-03, -2.3252e-06,\n",
      "        -1.3762e-04,  1.4942e-05, -5.2692e-05,  1.0026e-02,  8.8849e-03,\n",
      "        -1.1023e-04, -3.0860e-05, -9.2226e-05,  1.3127e-02,  8.5988e-03,\n",
      "         3.8229e-02])\n",
      "0.04418117079685722\n",
      "(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m minimal_ratio_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      5\u001b[0m train_losslist1\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 6\u001b[0m train_loss, minimal_ratio\u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m minimal_ratio_list\u001b[38;5;241m.\u001b[39mappend(minimal_ratio)\n\u001b[1;32m      9\u001b[0m train_losslist1\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(J[i], \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 41\u001b[0m     H[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([r\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result])\n\u001b[1;32m     43\u001b[0m eigenvalues, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msymeig(H, eigenvectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(eigenvalues)\n",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(J[i], \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 41\u001b[0m     H[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result])\n\u001b[1;32m     43\u001b[0m eigenvalues, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msymeig(H, eigenvectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(eigenvalues)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.RMSprop(model_1.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "loss_func = torch.nn.MSELoss()  # mean squared loss\n",
    "pytorch_total_params = sum(p.numel() for p in model_1.parameters())\n",
    "minimal_ratio_list=[]\n",
    "train_losslist1=[]\n",
    "train_loss, minimal_ratio= model_train(model_1, x, y)\n",
    "\n",
    "minimal_ratio_list.append(minimal_ratio)\n",
    "train_losslist1.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f1cce15-afef-4368-a9e0-9ba5bdaefb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model_1.parameters())\n",
    "print(num_param)\n",
    "names = list(n for n, _ in model_1.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da40df4e-6687-4efa-9bfb-323100f980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, x, y):\n",
    "    print(\"debug\")\n",
    "    loss_arr, minimal_list=[],[]\n",
    "    n_epochs = [*range(10)]\n",
    "    for epoch in range(1, len(n_epochs)+1):\n",
    "        print(\"debug\")\n",
    "        prediction = model(x)  \n",
    "        loss = loss_func(prediction, y)\n",
    "        optimizer.zero_grad() #clear gradients for next epoch\n",
    "        loss.backward() #compute gradients using back propogation\n",
    "        optimizer.step() \n",
    "        loss.requires_grad_()\n",
    "        print(f'epoch: {epoch}, loss = {loss.item():.4f}')\n",
    "        #J = torch.autograd.grad(loss, list(model.parameters()), retain_graph=True)\n",
    "        #J = torch.cat([e.flatten() for e in J]) # flatte\n",
    "        J = torch.cat([p.grad.flatten() for p in model.parameters()]) # flatten\n",
    "        print(J)\n",
    "        J.requires_grad_()\n",
    "        \n",
    "        grad_all=0.0\n",
    "        for p in model.parameters():\n",
    "            #print('debug3')\n",
    "            grad=0.0\n",
    "            if p.grad is not None:\n",
    "                #print('debug1')\n",
    "                grad= (p.grad.cpu().data.numpy()**2).sum()\n",
    "            grad_all+=grad\n",
    "        grad_norm=grad_all**0.5\n",
    "        #print('debug2')\n",
    "        print(grad_norm)\n",
    "        \n",
    "        if(grad_norm<3.50):\n",
    "            # Fill in Hessian\n",
    "            model.zero_grad()\n",
    "            H= torch.zeros((num_param, num_param))\n",
    "            for i in range(num_param):\n",
    "                #J[i].backward(retain_graph=True)\n",
    "                #H[i] = torch.cat([p.grad.flatten() for p in J[i]])\n",
    "                result = torch.autograd.grad(J[i], list(model.parameters()), retain_graph=True,allow_unused=True)\n",
    "                print(result)\n",
    "                H[i] = torch.cat([r.flatten() for r in result])\n",
    "                \n",
    "            eigenvalues, _ = torch.symeig(H, eigenvectors=False)\n",
    "            print(eigenvalues)\n",
    "            minimal_ratio = eigenvalues.min() / eigenvalues.sum()\n",
    "            print(minimal_ratio)\n",
    "            minimal_list.append(minimal_ratio)\n",
    "            loss_arr.append(loss.detach().numpy())\n",
    "            break\n",
    "        \n",
    "\n",
    "                \n",
    "    return loss_arr, minimal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a2465-bbc8-4b59-bde0-862af3e339f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losslist1 = []\n",
    "minimal_ratio_list=[]\n",
    "valid_loss_min = np.Inf\n",
    "for i in range(1):\n",
    "    n_epochs = [*range(10)]\n",
    "    valid_loss_min = np.Inf \n",
    "    plt.figure()\n",
    "    for epoch in range(1, len(n_epochs)+1):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        model1.train()\n",
    "        for data, target in train_loader:\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model1(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            loss.requires_grad_()\n",
    "            # Calculate Jacobian w.r.t. model parameters\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        model1.eval()\n",
    "        J = torch.cat([p.grad.flatten() for p in model1.parameters()]) # flatten\n",
    "        print(num_param)\n",
    "        print(J)\n",
    "        J.requires_grad_()\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "   \n",
    "        grad_all=0.0\n",
    "        for p in model1.parameters():\n",
    "            #print('debug3')\n",
    "            grad=0.0\n",
    "            if p.grad is not None:\n",
    "                #print('debug1')\n",
    "                grad= (p.grad.cpu().data.numpy()**2).sum()\n",
    "            grad_all+=grad\n",
    "        grad_norm=grad_all**0.5\n",
    "        #print('debug2')\n",
    "        print(grad_norm)\n",
    "        #plt.scatter(epoch,grad_norm)\n",
    "        # ---------------------------------minimal ratio-------------------------------------------- \n",
    "        if(grad_norm<3.50):\n",
    "            # Fill in Hessian\n",
    "            model1.zero_grad()\n",
    "            H= torch.zeros((num_param, num_param))\n",
    "            for i in range(num_param):\n",
    "                J[i].backward(retain_graph=True)\n",
    "                H[i] = torch.cat([p.grad.flatten() for p in model1.parameters()])\n",
    "                \n",
    "            eigenvalues, _ = torch.symeig(H, eigenvectors=False)\n",
    "            minimal_ratio = eigenvalues.min() / eigenvalues.sum()\n",
    "            minimal_ratio_list.append(minimal_ratio)\n",
    "            train_losslist1.append(train_loss)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # ---------------------------------minimal ratio------------------------------------------------ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        for data, target in valid_loader:\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model1(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        #train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "        \n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "            torch.save(model1.state_dict(), 'model_cifar.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "plt.scatter(minimal_ratio_list, train_losslist1)\n",
    "plt.xlabel(\"minimal ratio\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"minimal ration vs loss\")\n",
    "plt.show()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
